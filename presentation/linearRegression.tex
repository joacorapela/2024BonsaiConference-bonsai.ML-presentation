\begin{frame}
    \frametitle{Linear regression example}

	\begin{center}
		\includegraphics[width=2.2in]{figures/visVesIntegration.png}
	\end{center}
	\hfill\href{https://www.biorxiv.org/content/10.1101/2021.01.22.427789v4.abstract}{Keshavarzi et al., 2021}
	\begin{columns}
		\onslide<2->{
		\begin{column}{0.5\textwidth}
			\begin{center}
				\includegraphics[width=1.5in]{figures/spikeRateVsabsSpeedV1VisVes.png}
			\end{center}
		\end{column}
		}
		\onslide<3->{
		\begin{column}{0.5\textwidth}
			\textcolor{blue}{Is there a linear relation between the speed of rotation and the firing rate of visual cells?}
		\end{column}
		}
	\end{columns}
\end{frame}

\begin{frame}
    \frametitle{Estimating nonlinear receptive fields from natural images}

    \href{https://jov.arvojournals.org/article.aspx?articleid=2192869}{Rapela et al., 2006}.

\end{frame}

\begin{frame}
    \frametitle{Linear regression model}

	\scriptsize
	\begin{description}
		\item[simple linear regression model]
			\begin{align*}
                y(x_i, \mathbf{w})&=w_0+w_1x_i
				                   =\raisebox{0.60em}{$[1,x_i]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1
								    \end{array}\right]
				                   =\raisebox{0.60em}{$[\phi_0(x_i),\phi_1(x_i)]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1
								    \end{array}\right]\\
                                  & =
									\boldsymbol{\phi}(x_i)^\intercal\mathbf{w}
			\end{align*}
		\item[polynomial regression model]
			\begin{align*}
				y(x_i, \mathbf{w})&=w_0+w_1x_i+w_2x_i^2+w_3x_i^3
				                   =\raisebox{1.80em}{$[1,x_i,x_i^2,x_i^3]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1\\
								        w_2\\
								        w_3
								    \end{array}\right]\\
				                  &=\raisebox{1.80em}{$[\phi_0(x_i),\phi_1(x_i),\phi_2(x_i),\phi_3(x_i)]$}
									\left[\begin{array}{c}
								        w_0\\
								        w_1\\
								        w_2\\
								        w_3
								    \end{array}\right]=
									\boldsymbol{\phi}(x_i)^\intercal\mathbf{w}
			\end{align*}
		\item[basis functions linear regression model]
			\begin{align*}
				y(x_i, \mathbf{w})&=\boldsymbol{\phi}(x_i)^\intercal\mathbf{w}=\sum_{j=1}^Mw_j\phi_j(x_i)
			\end{align*}
	\end{description}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Linear regression model}

    \scriptsize
    \begin{align*}
        \mathbf{y}(\mathbf{x},\mathbf{w})&=
            \left[\begin{array}{c}
                      y(x_1,\mathbf{w})\\
                      y(x_2,\mathbf{w})\\
                      \ldots\\
                      y(x_N,\mathbf{w})
                  \end{array}\right]=
            \left[\begin{array}{cccc}
                      \phi_1(x_1)&\phi_2(x_1)&\ldots&\phi_M(x_1)\\
                      \phi_1(x_2)&\phi_2(x_2)&\ldots&\phi_M(x_2)\\
                      \vdots     &\vdots     &\ldots&\vdots\\
                      \phi_1(x_N)&\phi_2(x_N)&\ldots&\phi_M(x_N)
            \end{array}\right]\left[\begin{array}{c}
                                        w_1\\
                                        w_2\\
                                        \vdots\\
                                        w_M
                                    \end{array}\right]\\
                                         &=\boldsymbol{\Phi}\mathbf{w}
    \end{align*}

    where
    $\mathbf{y}(\mathbf{x},\mathbf{w})\in\mathbb{R}^N,\boldsymbol{\Phi}\in\mathbb{R}^{N\times M},\mathbf{w}\in\mathbb{R}^M$.
    \normalsize
\end{frame}

\begin{frame}
    \frametitle{Basis functions for regression}

		\begin{center}
			\includegraphics[width=3.5in]{figures/basisFunctions.png}
		\end{center}
        \hfill\scriptsize\citet{bishop06}

        \begin{description}
            \item[polynomial] $\phi_i(x)=x^i$
            \item[Gaussian] $\phi_i(x)=\exp(-\frac{(x-\mu_i)^2}{2\sigma^2})$
            \item[sigmoidal]
                $\phi_i(x)=\frac{1}{1+\exp(-\frac{x-\mu_i}{\sigma^2})}$
        \end{description}
\end{frame}

\begin{frame}
    \frametitle{Example dataset}

    \begin{center}
        \includegraphics[width=4in]{figures/exampleDataset.png}
    \end{center}
    \hfill\scriptsize\citet{bishop06}

\end{frame}

\subsection{Least-squares regression}

\begin{frame}
    \frametitle{Least-squares estimation of model parameters
    \citep{trefethenAndBau97}}

    \scriptsize
    \begin{probDef}[Least-squares problem]
        Given $\boldsymbol{\Phi}\in\mathbb{R}^{N\times M},N\ge
        M,\mathbf{t}\in\mathbb{R}^N$, find $\mathbf{w}\in\mathbb{R}^M$ such
        that $E_{LS}(\mathbf{w})=||\mathbf{t}-\boldsymbol{\Phi}\boldsymbol{w}||_2$ is minimised.
    \end{probDef}
    \begin{theorem}[Least-squares solution]
        Let $\boldsymbol{\Phi}\in\mathbb{R}^{N\times M} (N\ge M)$ and
        $\mathbf{t}\in\mathbb{R}^N$ be given. A vector
        $\mathbf{w}\in\mathbb{R}^M$ minimises
        $||\mathbf{r}||_2=||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2$, thereby solving the
        least-squares problem, if and only if
        $\mathbf{r}\perp\text{range}(\boldsymbol{\Phi})$, that is,
        $\boldsymbol{\Phi}^\intercal\mathbf{r}=0$,
        or equivalently,
        $\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}\mathbf{w}=\boldsymbol\Phi^\intercal\mathbf{t}$,
        or again equivalently,
        $P\mathbf{t}=\boldsymbol{\Phi}\mathbf{w}$,
        where $P\in\mathbf{R}^{N\times N}$ is the orthogonal projector onto
        $\text{range}(A)$ (i.e., $P=A\;(A^\intercal A)^{-1}A^\intercal)$.

    \end{theorem}
	\begin{center}
		\includegraphics[width=4in]{figures/leastSquares.png}
	\end{center}
    \hfill\scriptsize\citet{bishop06}
    \normalsize

    \note{
    Given a set of $N$ observations, $\mathbf{t}$, $N>M$, we want to find model
    parameters $\mathbf{w}$ such that the model outputs,
    $\mathbf{t}(\mathbf{x},\mathbf{w})$ equal the observations. This is
    generally impossible, because the degrees of freedom of the observations,
    $N$, is generally larger than the degrees of freedom of the model
    $\mathbf{t}(\mathbf{x},\mathbf{w})$, $M$. We instead solve the following
    least-squares problem.
    }
\end{frame}

\begin{frame}[fragile]
    \frametitle{Instruction to run notebooks in Google Colab}

    \begin{enumerate}
        \item open a notebook from
            \href{https://github.com/joacorapela/gcnuBridging2023/tree/master/docs/sphinx/build/html/notebooks/auto_examples/bayesianLinearRegression}{here}
        \item replace \textbf{github.com} by \textbf{githubtocolab.com} in the
            URL
        \item insert a cell at the beginning of the notebook with the following
            content
        \seti
    \end{enumerate}

    \tiny
    \begin{verbatim}
       !git clone https://github.com/joacorapela/gcnuBridging2023.git
       %cd gcnuBridging2023
       !pip install -e .
    \end{verbatim}
    \normalsize

    \begin{enumerate}
        \conti
        \item from the menu \textbf{Runtime} select \textbf{Run all}.
    \end{enumerate}
\end{frame}

\begin{frame}
    \frametitle{Code for least-squares estimation of model parameters}

    \begin{itemize}
        \item \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotOverfittingLeastSquares.html\#sphx-glr-auto-examples-bayesianlinearregression-plotoverfittingleastsquares-py}{overfitting}
        \item \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotCrossValidationLeastSquares.html\#sphx-glr-auto-examples-bayesianlinearregression-plotcrossvalidationleastsquares-py}{cross validation}
        \item \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotLackOfOverfittingInLeastSquaresForLargerDatasetSize.html\#sphx-glr-auto-examples-bayesianlinearregression-plotlackofoverfittinginleastsquaresforlargerdatasetsize-py}{larger datasets allow more complex models}
    \end{itemize}

\end{frame}

\begin{frame}
    \frametitle{Regularised least-squares estimation of model parameters}

    To cope with the overfitting of least squares, we can add to the least
    squares optimisation criterion a term that enforces coefficients to be
    zero. The regularised least-squares optimisation criterion becomes:

    \begin{align*}
        E_{RLS}(\mathbf{w})=||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2+\lambda||\mathbf{w}||_2^2
    \end{align*}

    where $\lambda$ is the regularisation parameter that weights the strength
    of the regularisation.
\end{frame}

\begin{frame}
    \frametitle{Regularised least-squares estimation of model parameters}
	\scriptsize
	\begin{claim}[Regularised least-squares estimate]
		\begin{align*}
			\mathbf{w}_{RLS}=\argmin_{\mathbf{w}}E_{RLS}(\mathbf{w})=\argmin_{\mathbf{w}}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2+\lambda||\mathbf{w}||_2^2=(\lambda\mathbf{I}+\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t}
		\end{align*}
	\end{claim}
	\tiny
	\begin{proof}
		Since $E_{RLS}(\mathbf{w})$ is a polynomial of order two on the elements of $\mathbf{w}$ (i.e., a quadratic form), we can use the \emph{Completing the Squares} technique below to find its minimum.
		\begin{align}
			\boldsymbol{\mu}&=\argmax_{\mathbf{w}}\mathcal{N}(\mathbf{w}|\boldsymbol{\mu},\Sigma)=\argmax_{\mathbf{w}}\log\mathcal{N}(\mathbf{w}|\boldsymbol{\mu},\Sigma)\nonumber\\
                            &=\argmax_{\mathbf{w}}\{K-\frac{1}{2}(-2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w})\}\label{eq:completingTheSquaresStep2}\\
                            &=\argmin_{\mathbf{w}}\{-K+\frac{1}{2}(-2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w})\}\nonumber\\
                            &=\argmin_{\mathbf{w}}\{K_1-2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w}\}\label{eq:completingTheSquares}
		\end{align}

		% Note: Eq.~\ref{eq:completingTheSquaresStep2} uses Eq.~\ref{eq:gaussianQuadratic}.

		To find the minimum of a quadratic form, we write it in the form of the
terms inside the curly brackets of Eq.~\ref{eq:completingTheSquares}, and the
term corresponding to $\boldsymbol{\mu}$ will be the minimum.

		\phantom\qedhere
	\end{proof}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Regularised least-squares estimation of model parameters}
	\tiny
		\begin{proof}
			Let's write $E_{RLS}$ in the form of the terms inside the curly brackets of Eq.~\ref{eq:completingTheSquares}.

		\begin{align*}
			E_{RLS}&=||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2+\lambda||\mathbf{w}||_2^2=(\mathbf{t}-\boldsymbol{\Phi}\mathbf{w})^\intercal(\mathbf{t}-\boldsymbol{\Phi}\mathbf{w})+\lambda\mathbf{w}^\intercal\mathbf{w}\\
                   &=\mathbf{t}^\intercal\mathbf{t}-2\mathbf{t}^\intercal\boldsymbol{\Phi}\mathbf{w}+\mathbf{w}^\intercal\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}\mathbf{w}+\lambda\mathbf{w}^\intercal\mathbf{w}\\
                   &=\mathbf{t}^\intercal\mathbf{t}-2\mathbf{t}^\intercal\boldsymbol{\Phi}\mathbf{w}+\mathbf{w}^\intercal(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}+\lambda\mathbf{I}_M)\mathbf{w}
		\end{align*}
		Calling
		\begin{align*}
			\Sigma^{-1}&=\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}+\lambda\mathbf{I}_M\\
			\boldsymbol{\mu}^\intercal\Sigma^{-1}&=\mathbf{t}^\intercal\boldsymbol{\Phi}\;\text{or}\;\boldsymbol{\mu}^\intercal=\mathbf{t}^\intercal\boldsymbol{\Phi}\Sigma\;\text{or}\;\boldsymbol{\mu}=\Sigma\boldsymbol{\Phi}^\intercal\mathbf{t}=\left(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}+\lambda\mathbf{I}_M\right)^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t}
		\end{align*}
		we can express
		\begin{align*}
			E_{RLS}=K+2\boldsymbol{\mu}^\intercal\Sigma^{-1}\mathbf{w}+\mathbf{w}\Sigma^{-1}\mathbf{w}
		\end{align*}
		Then
		\begin{align*}
			 \mathbf{w}_{RLS}=\argmin_{\mathbf{w}}E_{RLS}(\mathbf{w})=\boldsymbol{\mu}=\left(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}+\lambda\mathbf{I}_M\right)^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t} 
		\end{align*}
		\end{proof}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Code for regularised least-squares estimation of model parameters}
    \begin{itemize}
        \item \href{file:///nfs/ghome/live/rapela/dev/teaching/gcnuBridging2023/repo/docs/sphinx/build/html/auto\_examples/bayesianLinearRegression/plotRegularizedLeastSquares.html\#sphx-glr-auto-examples-bayesianlinearregression-plotregularizedleastsquares-py}{control of overfitting}
    \end{itemize}
\end{frame}

\subsection{Maximum-likelihood regression}

\begin{frame}
    \frametitle{Maximum-likelihood estimation of model parameters}

	\scriptsize
    \begin{probDef}[Likelihood function]
        For a statistical model characterised by a probability density
        function $p(\mathbf{x}|\theta)$ (or probability mass function
        $P_\theta(X=\mathbf{x})$) the likelihood function is a function of the
        parameters $\theta$, $\mathcal{L}(\theta)=p(\mathbf{x}|\theta)$
        (or $\mathcal{L}(\theta)=P_\theta(\mathbf{x})$).
   \end{probDef}

    \begin{probDef}[Maximum likelihood parameters estimates]
        The maximum likelihood parameters estimates are the parameters that
        maximise the likelihood function.

        \begin{align*}
            \theta_{ML}=\argmax_{\theta}\mathcal{L}(\theta)
        \end{align*}
   \end{probDef}

	\normalsize

\end{frame}

\begin{frame}
    \frametitle{Maximum-likelihood estimation for the basis function linear
    regression model}

    \footnotesize
    We seek the parameter $\mathbf{w}_{ML}$ and $\beta_{ML}$ that maximised the following likelihood function

    \begin{align}
        \mathcal{L}(\mathbf{w},\beta)=p(\mathbf{t}|\mathbf{w},\beta)=\mathcal{N}(\mathbf{t}|\boldsymbol{\Phi}\mathbf{w},\beta^{-1}I_N)\label{eq:likelihoodLinearRegression}
    \end{align}

    They are

    \begin{align}
        \mathbf{w}_{ML}&=(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t}\label{eq:wML}\\
        \frac{1}{\beta_{ML}}&=\frac{1}{N}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}_{ML}||_2^2\label{eq:betaML}
    \end{align}

	\begin{itemize}

		\item first regression method that assumes random observations

		\item if the likelihood function is assumed to be Normal,
		maximum-likelihood and least-squares coefficients estimates are equal.

	\end{itemize}

    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Maximum likelihood: exercise}

    \scriptsize
    \begin{probExercise}
        Derive the formulas for the maximum likelihood estimates of the
        coefficients, $\mathbf{w}$, and noise precision, $\beta$, of the basis
        functions linear regression model given in Eqs.~\ref{eq:wML}
        and~\ref{eq:betaML}.
    \end{probExercise}

    \tiny
    \begin{proof}[Solution]
        \begin{align*}
            \mathcal{L}(\mathbf{w},\beta)&=p(\mathbf{t}|\mathbf{w},\beta)=\mathcal{N}(\mathbf{t}|\boldsymbol{\Phi}\mathbf{w},\beta^{-1}\mathbf{I})\\
                                         &=\frac{1}{(2\pi)^\frac{N}{2}|\beta^{-1}\mathbf{I}|^\frac{1}{2}}\exp\left(-\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2\right)\\
            \log\mathcal{L}(\mathbf{w},\beta)&=-\frac{N}{2}\log{2\pi}+\frac{N}{2}\log\beta-\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2\\
            \mathbf{w}_{ML}&=\argmax_{\mathbf{w}}\log\mathcal{L}(\mathbf{w},\beta)=\argmin_{\mathbf{w}}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2=(\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi})^{-1}\boldsymbol{\Phi}^\intercal\mathbf{t}\\
            \frac{\partial}{\partial\beta}\log p(\mathbf{t}|\mathbf{w}_{ML},\beta)&=\frac{N}{2}\frac{1}{\beta}-\frac{1}{2}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}_{ML}||_2^2\\
            \frac{\partial}{\partial\beta}\log
            p(\mathbf{t}|\mathbf{w}_{ML},\beta_{ML})&=0\quad\text{iff}\quad\frac{1}{\beta_{ML}}=\frac{1}{N}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}_{ML}||_2^2
        \end{align*}
		\phantom\qedhere
    \end{proof}
    \normalsize
\end{frame}

\subsection{Bayesian linear regression}

\begin{frame}
    \frametitle{Bayesian linear regression: motivation}

	\begin{itemize}
		\item elegant,
		\item naturally leads to online regression,
		\item does not require cross-validation for model selection,
		\item it is the first step to more complex Bayesian modelling.
	\end{itemize}

\end{frame}

\subsubsection{Batch Bayesian linear regression}

\begin{frame}
    \frametitle{Batch Bayesian linear regression: posterior distribution of parameters}

	\scriptsize
	In Bayesian linear regression we seek the posterior distribution of the
weights of the linear regression model, $\mathbf{w}$, given the observations, which
is proportional to the product of the likelihood function,
$p(\mathbf{t}|\mathbf{w})$, and the prior, $p(\mathbf{w})$; i.e., 

	\begin{align}
		p(\mathbf{w}|\mathbf{t})\propto
        p(\mathbf{t}|\mathbf{w})p(\mathbf{w})\label{eq:priorLinearRegression}
	\end{align}

	To calculate this posterior below we use the likelihood function defined in
Eq.~\ref{eq:likelihoodLinearRegression} and the following prior

	\begin{align*}
		p(\mathbf{w})=\mathcal{N}(\mathbf{w}|\mathbf{0},\alpha^{-1}\mathbf{I})
	\end{align*}

	Using the expression of the conditional of the Linear Gaussian model,
Eq.~\ref{eq:conditionalLinearGaussianModel}, we obtain

	\begin{align}
		p(\mathbf{w}|\mathbf{t})&=\mathcal{N}(\mathbf{w}|\mathbf{m}_N,\mathbf{S}_N)\nonumber\\
		\mathbf{m}_N&=\beta\mathbf{S}_N\boldsymbol{\Phi}^\intercal\mathbf{t}\label{eq:blrPosteriorMean}\\
		\mathbf{S}_N^{-1}&=\alpha\mathbf{I}+\beta\boldsymbol{\Phi}^\intercal\boldsymbol{\Phi}\label{eq:blrPosteriorCov}
	\end{align}

	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Batch Bayesian linear regression: exercise}

    \scriptsize
    \begin{probExercise}
		Derive the formulas for the Bayesian posterior mean
(Eq.~\ref{eq:blrPosteriorMean}) and covariance (Eq.~\ref{eq:blrPosteriorCov})
of the basis function linear regression model.
    \end{probExercise}

    \begin{probExercise}
        Show that

        \begin{align}
            \log p(\mathbf{w}|\boldsymbol{t})&=K-\frac{\beta}{2}||\mathbf{t}-\boldsymbol{\Phi}\mathbf{w}||_2^2-\frac{\alpha}{2}||\mathbf{w}||_2^2
        \end{align}

        Therefore, the maximum-a-posteriori parameters of the basis function
        linear regression model are the solution of the regularised
        least-squares problem with $\lambda=\alpha/\beta$.

        Note that, as we will show next, Bayesian linear regression uses the
        full posterior of the parameters to make predictions or to do model
        selection, and not just the maximum-a-posteriori parameters.

    \end{probExercise}
    \normalsize

\end{frame}

\begin{frame}
    \frametitle{Batch Bayesian linear regression: demo code}
    Available \href{https://joacorapela.github.io/gcnuBridging2023/auto\_examples/bayesianLinearRegression/plotBatchBayesianLinearRegression.html\#sphx-glr-auto-examples-bayesianlinearregression-plotbatchbayesianlinearregression-py}{here}
\end{frame}

\subsubsection{Online Bayesian linear regression}

\begin{frame}
    \frametitle{Online Bayesian linear regression: recursive update of posterior distribution of parameters}
	\scriptsize
	\begin{claim}[recursive update]
		If the observations, $\{\mathbf{t}_1,\ldots,\mathbf{t}_n,\dots\}$, are linearly independent when conditioned on the model parameters, $\boldsymbol{\theta}$, then for any $n\in\mathbb{N}$
		\begin{align}
			p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)=K\ p(\mathbf{t}_n|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_{n-1})
		\end{align}
		where $K$ is a quantity that does not depend on $\boldsymbol{\theta}$.
	\end{claim}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{Online Bayesian linear regression: recursive update of posterior distribution of parameters}
	\tiny
	\begin{proof}
		By induction on $H_n: p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)=K\ p(\mathbf{t}_n|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_{n-1})$.
		\begin{description}
			\item[$H_1$]
				\begin{align*}
					p(\boldsymbol{\theta}|\mathbf{t}_1)=\frac{p(\boldsymbol{\theta},\mathbf{t}_1)}{p(\mathbf{t}_1)}=\frac{p(\mathbf{t}_1|\boldsymbol{\theta})p(\boldsymbol{\theta})}{p(\mathbf{t}_1)}=K\ p(\mathbf{t}_1|\boldsymbol{\theta})p(\boldsymbol{\theta})
				\end{align*}
			\item[$H_n\rightarrow H_{n+1}$]
				\begin{align*}
					p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})&=\frac{p(\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})}{p(\mathbf{t}_1,\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=\frac{p(\mathbf{t}_{n+1}|\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_n)p(\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_n)}{p(\mathbf{t}_1\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=\frac{p(\mathbf{t}_{n+1}|\boldsymbol{\theta})p(\boldsymbol{\theta},\mathbf{t}_1,\ldots,\mathbf{t}_n)}{p(\mathbf{t}_1\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=\frac{p(\mathbf{t}_{n+1}|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)p(\mathbf{t}_1,\ldots,\mathbf{t}_n)}{p(\mathbf{t}_1\ldots,\mathbf{t}_{n+1})}\\
                                                                               &=K\ p(\mathbf{t}_{n+1}|\boldsymbol{\theta})p(\boldsymbol{\theta}|\mathbf{t}_1,\ldots,\mathbf{t}_n)
				\end{align*}
				Note: the third equality above holds because the observations are assumed to be conditional independent given the parameters.
		\end{description}
	\end{proof}
	\normalsize
\end{frame}

\begin{frame}
    \frametitle{References}

    \tiny{
        \bibliographystyle{apalike}
        \bibliography{probability,informationTheory,machineLearning,gaussianProcesses,latentsVariablesModels,linearDynamicalSystems,numericalMethods}
    }
\end{frame}

\end{document}

    \end{probExercise}
    We integrate Eq.~\ref{eq:modelComparison} using the expression for the
    marginal of the linear Gaussian model,
    Eq.~\ref{eq:marginalLinearGaussianModel}, obtaining

    \begin{align}
        p(\mathbf{t}|\alpha,\beta)=\mathcal{N}(\mathbf{t}|\mathbf{0},\alpha^{-1}\boldsymbol{\phi}\boldsymbol{\Phi}^\intercal+\beta^{-1}\mathbf{I}_N)
    \end{align}

    \normalsize
\end{frame}

\begin{frame}
    \frametitle{References}

    \tiny{
        \bibliographystyle{apalike}
        \bibliography{probability,informationTheory,machineLearning,gaussianProcesses,latentsVariablesModels,linearDynamicalSystems,numericalMethods}
    }
\end{frame}

\end{document}

